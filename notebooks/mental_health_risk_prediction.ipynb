{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd93cgPX-qod"
      },
      "source": [
        "# Digital Phenotyping and Mental Health Risk Prediction Using Wearable Device Data\n",
        "\n",
        "### --- Introduction ---\n",
        "### This notebook follows a methodological approach to explore, preprocess, and model data collected from wearable devices and mental health questionnaires. The goal is to predict mental health risk using physiological data and advanced machine learning techniques.\n",
        "\n",
        "**Table of Contents:**\n",
        "1. [Setup and Library Installation](#setup)\n",
        "2. [Data Loading and Exploratory Data Analysis (EDA)](#eda)\n",
        "3. [Data Preprocessing](#preprocessing)\n",
        "4. [Data Transformation](#transformation)\n",
        "5. [Machine Learning Model Development](#modeling)\n",
        "6. [Model Evaluation and Visualization](#evaluation)\n",
        "7. [Model Interpretability](#interpretability)\n",
        "8. [Alternative Modeling with Dimensionality Reduction](#dimensionality)\n",
        "9. [Comparison of Models with Different PCA Components](#comparison)\n",
        "10. [Data Integrity and Minimization](#integrity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfZmFoT2CBRp"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## 1. Setup and Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5c5nTj-pcD",
        "outputId": "0ec08a3b-ab0c-4396-cf0e-6ccbceb0414e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN7xI1Kz-y3z"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries quietly to avoid cluttering the output\n",
        "!pip install pyts seaborn dask tf-keras-vis shap lime fancyimpute --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA5rZr64-7ws"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data manipulation and visualization\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import timedelta\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Import libraries for data preprocessing and modeling\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "from sklearn.utils import shuffle\n",
        "from pyts.image import GramianAngularField\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Import libraries for building the neural network\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Import libraries for interpretability\n",
        "import shap\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "# Import MICE imputation method from scikit-learn\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plot styles for better aesthetics\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_2YY22N-7tW"
      },
      "outputs": [],
      "source": [
        "# Base path to data files in Google Drive\n",
        "base_path = \"/content/drive/MyDrive/stress_master/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yn2BxSj-7qP"
      },
      "outputs": [],
      "source": [
        "# Questionnaire files and their respective timestamp columns\n",
        "questionnaire_files_info = {\n",
        "    \"patient_health_questionnaire_phq9.csv\": \"phq9_ts\",\n",
        "    \"generalized_anxiety_disorder_scale_gad7.csv\": \"gad_ts\",\n",
        "    \"perceived_stress_scale_pss4.csv\": \"pss4_ts\"\n",
        "}\n",
        "\n",
        "# Digital Health Technology (DHT) files (Physiological Data from Wearable Devices)\n",
        "dht_files = [\n",
        "    \"garmin_epoch_run.csv\",\n",
        "    \"oura_extension_readiness.csv\",\n",
        "    \"oura_readiness.csv\",\n",
        "    \"garmin_epoch_walk.csv\",\n",
        "    \"oura_extension_sleep.csv\",\n",
        "    \"garmin_epoch_idle.csv\",\n",
        "    \"oura_sleep.csv\",\n",
        "    \"oura_extension_activity.csv\"\n",
        "]\n",
        "\n",
        "# Valid participant IDs (ensures consistency across datasets)\n",
        "valid_ids = {\n",
        "    'U-B77UDKRYXV85ESYFKNQ5', 'U-5U51AGTFQ3ZQP6LHASV8', 'U-BKQPVE7SQ6S4E3F2TCAT',\n",
        "    'U-GS4NJ3G9ZVUETDF1847D', 'U-V1TJFFPQALNM9G3LRTSP', 'U-ZW5VUY46VKDERT8XM6Z4',\n",
        "    'U-HMS81C96UB7MPJJ4CSTH', 'U-8RBGSLAPG8YLY6FHJW75', 'U-J692A9AKCT7LM13RVH1K',\n",
        "    'U-QWKQ632DVK5WBMPQTL3M', 'U-D6M4TDMFSD1WQL88LRN8', 'U-W41B6L41VS1LGYMNPRCU',\n",
        "    'U-PZ5PSRW2SVQHNDNNLM7W', 'U-S1JFKPZWUG7K2L6DMBQU', 'U-X8Q3A3Z18ZBAXJ6DMZ5W',\n",
        "    'U-8ZKFQ6KLTMU17TT6DBF1', 'U-SWLQR3AXMCT7KURBMV8H', 'U-YY1J4CAB6VNDKUFHJ99E',\n",
        "    'U-3N1NZWHWN7Y96Z3Z1A9J', 'U-8CM21J68ZTPZHRH5REXV', 'U-TA9DZH13P9C7LWU9DRTN',\n",
        "    'U-FY2M2PN4EFRQYN9R5CXY', 'U-QZLLZBKC13LNFT9NZASU', 'U-L57KAZZ2HNALN6UZW1PV',\n",
        "    'U-YT5AF8GSQA3WKACCU6H8', 'U-T4GSS7ESBLQWRCYT4RDX', 'U-6N9FECGL19X1E4LXBSM3',\n",
        "    'U-XZ33PFUXKV7WPYR3XGC7', 'U-NZKTR5L6XXPT4BNQF3NQ', 'U-7KJCJE27RESNM2M74TF4',\n",
        "    'U-ZUY395L4QK8TQXMLTJNH', 'U-1M1PWFRM99UKVZBE4JN8', 'U-51YQ41XX3C54B86EM775',\n",
        "    'U-SSQ22TL31ZVQ5FT87FYA', 'U-D9UX49JDSB2RK49X12HS', 'U-FZFXQL6T5J3K7QHZJRTA',\n",
        "    'U-J8E38GJFGUJBMY7HFQ4V', 'U-ACCEE9K6JQL6XBHFRZHE', 'U-XZ7U958ZLVPHTUVFSZW7',\n",
        "    'U-X4XN7213NM6V52ERNC3L', 'U-YY1AMRSCJ1N8FTJE8YH6', 'U-XLBUMB67W4MZCG847GZ9',\n",
        "    'U-7ZYS5MWSE339X5Q4PFPM', 'U-YQEHZM1ZP3XCSHVBP11V', 'U-DBY492Q47FNB3C68CHC1',\n",
        "    'U-GFN8A66UFG8K3UFX3XQK', 'U-B9EERJ5XGKFNXMUP76P6', 'U-EAJF8BV1CZV7TU8Q4A97',\n",
        "    'U-BL3W3BNN4RNC4Z29DZRS', 'U-V9DDVJLUL1N7BSYMLW76', 'U-JQPCQ457SWNQKQPHBARR',\n",
        "    'U-7N4QAAKGRRXGV7V66NSG'\n",
        "}\n",
        "\n",
        "# Define feature sets for HRV and sleep metrics\n",
        "hrv_columns = ['score_hrv_balance', 'rmssd', 'hr_lowest']\n",
        "sleep_columns = ['score_sleep_balance', 'score_total', 'deep', 'light', 'rem', 'efficiency', 'onset_latency', 'hr_average']\n",
        "outcome_columns = ['phq9_total', 'gad_total', 'pss4_total']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8PRgK6Y_HAa"
      },
      "source": [
        "<a name=\"eda\"></a>\n",
        "## 2. Data Loading and Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, we will load the questionnaire and DHT data, and perform initial exploratory data analysis to understand the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpgX4uBk-7nk"
      },
      "outputs": [],
      "source": [
        "# Function to load questionnaire data efficiently\n",
        "def load_questionnaire_files(base_path, questionnaire_files_info, chunksize=500000):\n",
        "    questionnaire_dfs = []\n",
        "    for file, timestamp_col in questionnaire_files_info.items():\n",
        "        file_path = os.path.join(base_path, file)\n",
        "        if not os.path.exists(file_path):\n",
        "            continue  # Skip if the file doesn't exist\n",
        "        try:\n",
        "            # Read data in chunks to manage memory usage\n",
        "            chunk_iter = pd.read_csv(\n",
        "                file_path,\n",
        "                chunksize=chunksize,\n",
        "                dtype={'participant_id': 'object'},\n",
        "                parse_dates=[timestamp_col]\n",
        "            )\n",
        "            for chunk in chunk_iter:\n",
        "                questionnaire_dfs.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "    if not questionnaire_dfs:\n",
        "        return pd.DataFrame()\n",
        "    merged_questionnaires = pd.concat(questionnaire_dfs, ignore_index=True)\n",
        "    # Compute total scores for questionnaires if not already present\n",
        "    if 'phq9_total' not in merged_questionnaires.columns and any(col.startswith('phq9_') for col in merged_questionnaires.columns):\n",
        "        phq9_cols = [col for col in merged_questionnaires.columns if col.startswith('phq9_') and col != 'phq9_ts']\n",
        "        merged_questionnaires['phq9_total'] = merged_questionnaires[phq9_cols].sum(axis=1)\n",
        "    if 'gad_total' not in merged_questionnaires.columns and any(col.startswith('gad_') for col in merged_questionnaires.columns):\n",
        "        gad_cols = [col for col in merged_questionnaires.columns if col.startswith('gad_') and col != 'gad_ts']\n",
        "        merged_questionnaires['gad_total'] = merged_questionnaires[gad_cols].sum(axis=1)\n",
        "    if 'pss4_total' not in merged_questionnaires.columns and any(col.startswith('pss4_') for col in merged_questionnaires.columns):\n",
        "        pss4_cols = [col for col in merged_questionnaires.columns if col.startswith('pss4_') and col != 'pss4_ts']\n",
        "        merged_questionnaires['pss4_total'] = merged_questionnaires[pss4_cols].sum(axis=1)\n",
        "    return merged_questionnaires\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-QbbHQx-7kp"
      },
      "outputs": [],
      "source": [
        "# Function to load DHT data efficiently\n",
        "def load_dht_files(base_path, dht_files, chunksize=500000):\n",
        "    dht_dfs = []\n",
        "    for file in dht_files:\n",
        "        file_path = os.path.join(base_path, file)\n",
        "        if not os.path.exists(file_path):\n",
        "            continue  # Skip if the file doesn't exist\n",
        "        try:\n",
        "            # Read data in chunks to manage memory usage\n",
        "            chunk_iter = pd.read_csv(\n",
        "                file_path,\n",
        "                chunksize=chunksize,\n",
        "                dtype={'participant_id': 'object'},\n",
        "                parse_dates=['summary_date']\n",
        "            )\n",
        "            for chunk in chunk_iter:\n",
        "                dht_dfs.append(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "    if not dht_dfs:\n",
        "        return pd.DataFrame()\n",
        "    merged_dht = pd.concat(dht_dfs, ignore_index=True)\n",
        "    return merged_dht\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3vpJC5--7gy"
      },
      "outputs": [],
      "source": [
        "# Load questionnaire data\n",
        "questionnaire_data = load_questionnaire_files(base_path, questionnaire_files_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79UuRszk-7eL"
      },
      "outputs": [],
      "source": [
        "# Load DHT data\n",
        "dht_data = load_dht_files(base_path, dht_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHL7U5bS_SAN"
      },
      "source": [
        "<a name=\"preprocessing\"></a>\n",
        "## 3. Data Preprocessing\n",
        "\n",
        "We will preprocess the data by merging datasets, handling missing values using MICE imputation, standardizing timestamps, and creating risk labels based on questionnaire scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V33rQP6o-7bo"
      },
      "outputs": [],
      "source": [
        "# Merge and clean data only if both datasets are available\n",
        "if not questionnaire_data.empty and not dht_data.empty:\n",
        "    # Select relevant columns from questionnaire data\n",
        "    questionnaire_columns = ['participant_id', 'phq9_ts', 'gad_ts', 'pss4_ts', 'phq9_total', 'gad_total', 'pss4_total']\n",
        "    available_questionnaire_columns = [col for col in questionnaire_columns if col in questionnaire_data.columns]\n",
        "    questionnaire_data = questionnaire_data[available_questionnaire_columns]\n",
        "\n",
        "    # Select relevant columns from DHT data\n",
        "    selected_dht_columns = ['participant_id', 'summary_date'] + hrv_columns + sleep_columns\n",
        "    available_dht_columns = [col for col in selected_dht_columns if col in dht_data.columns]\n",
        "    dht_data = dht_data[available_dht_columns]\n",
        "\n",
        "    # Merge datasets on 'participant_id'\n",
        "    merged_data = pd.merge(\n",
        "        dht_data,\n",
        "        questionnaire_data,\n",
        "        on='participant_id',\n",
        "        how='inner'  # Ensures only participants present in both datasets are included\n",
        "    )\n",
        "    # Filter for valid participant IDs\n",
        "    merged_data = merged_data[merged_data['participant_id'].isin(valid_ids)]\n",
        "    # Drop rows where all critical timestamps are missing\n",
        "    critical_columns = ['summary_date', 'phq9_ts', 'gad_ts', 'pss4_ts']\n",
        "    merged_data.dropna(subset=critical_columns, how='all', inplace=True)\n",
        "else:\n",
        "    merged_data = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Ts3LRwanQl",
        "outputId": "6a12515f-3aee-43bc-8ff1-df68ac07b3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'summary_date' column successfully renamed to 'timestamp_dht'.\n",
            "Columns after renaming: ['participant_id', 'timestamp_dht', 'score_hrv_balance', 'rmssd', 'hr_lowest', 'score_sleep_balance', 'score_total', 'deep', 'light', 'rem', 'efficiency', 'onset_latency', 'hr_average', 'phq9_ts', 'gad_ts', 'pss4_ts', 'phq9_total', 'gad_total', 'pss4_total']\n",
            "Merged data shape after timestamp processing: (11589006, 19)\n",
            "Columns after timestamp processing: ['participant_id', 'timestamp_dht', 'score_hrv_balance', 'rmssd', 'hr_lowest', 'score_sleep_balance', 'score_total', 'deep', 'light', 'rem', 'efficiency', 'onset_latency', 'hr_average', 'phq9_ts', 'gad_ts', 'pss4_ts', 'phq9_total', 'gad_total', 'pss4_total']\n",
            "Merged data shape before filtering valid IDs: (11589006, 19)\n",
            "Merged data shape after filtering valid IDs: (5965093, 19)\n",
            "Columns after filtering valid IDs: ['participant_id', 'timestamp_dht', 'score_hrv_balance', 'rmssd', 'hr_lowest', 'score_sleep_balance', 'score_total', 'deep', 'light', 'rem', 'efficiency', 'onset_latency', 'hr_average', 'phq9_ts', 'gad_ts', 'pss4_ts', 'phq9_total', 'gad_total', 'pss4_total']\n",
            "'timestamp_dht' column is still present after filtering.\n"
          ]
        }
      ],
      "source": [
        "# Merge datasets\n",
        "merged_data = pd.merge(\n",
        "    dht_data,\n",
        "    questionnaire_data,\n",
        "    on='participant_id',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Strip whitespace from column names\n",
        "merged_data.columns = merged_data.columns.str.strip()\n",
        "\n",
        "# Rename 'summary_date' to 'timestamp_dht'\n",
        "if 'summary_date' in merged_data.columns:\n",
        "    merged_data.rename(columns={'summary_date': 'timestamp_dht'}, inplace=True)\n",
        "    print(\"'summary_date' column successfully renamed to 'timestamp_dht'.\")\n",
        "else:\n",
        "    print(\"'summary_date' column not found after stripping whitespace.\")\n",
        "\n",
        "# Verify columns after renaming\n",
        "print(\"Columns after renaming:\", merged_data.columns.tolist())\n",
        "\n",
        "# Process timestamps\n",
        "if 'timestamp_dht' in merged_data.columns:\n",
        "    timestamp_cols = ['timestamp_dht', 'phq9_ts', 'gad_ts', 'pss4_ts']\n",
        "    for col in timestamp_cols:\n",
        "        if col in merged_data.columns:\n",
        "            merged_data[col] = pd.to_datetime(merged_data[col], utc=True, errors='coerce')\n",
        "            merged_data[col] = merged_data[col].dt.floor('D')  # Retain only the date part\n",
        "    # Drop rows with missing 'timestamp_dht'\n",
        "    merged_data.dropna(subset=['timestamp_dht'], inplace=True)\n",
        "    print(\"Merged data shape after timestamp processing:\", merged_data.shape)\n",
        "else:\n",
        "    print(\"'timestamp_dht' column is missing after renaming. Cannot proceed.\")\n",
        "\n",
        "# Verify columns after timestamp processing\n",
        "print(\"Columns after timestamp processing:\", merged_data.columns.tolist())\n",
        "\n",
        "# Filter for valid participant IDs\n",
        "print(\"Merged data shape before filtering valid IDs:\", merged_data.shape)\n",
        "merged_data = merged_data[merged_data['participant_id'].isin(valid_ids)]\n",
        "print(\"Merged data shape after filtering valid IDs:\", merged_data.shape)\n",
        "\n",
        "# Verify columns after filtering\n",
        "print(\"Columns after filtering valid IDs:\", merged_data.columns.tolist())\n",
        "\n",
        "# Check if 'timestamp_dht' is still present\n",
        "if 'timestamp_dht' in merged_data.columns:\n",
        "    print(\"'timestamp_dht' column is still present after filtering.\")\n",
        "else:\n",
        "    print(\"'timestamp_dht' column is missing after filtering.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT8pKOB1gk-d",
        "outputId": "680f0c2b-ff9c-4685-bc87-402ef26c553f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in timestamp_dht after conversion: 0\n",
            "Missing values in phq9_ts after conversion: 11103618\n",
            "Missing values in gad_ts after conversion: 11122210\n",
            "Missing values in pss4_ts after conversion: 10672671\n",
            "Shape after dropping missing 'timestamp_dht': (11589006, 19)\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'summary_date' is correctly renamed to 'timestamp_dht'\n",
        "if 'summary_date' in merged_data.columns:\n",
        "    merged_data.rename(columns={'summary_date': 'timestamp_dht'}, inplace=True)\n",
        "\n",
        "# Convert timestamps and check for missing values\n",
        "timestamp_cols = ['timestamp_dht', 'phq9_ts', 'gad_ts', 'pss4_ts']\n",
        "for col in timestamp_cols:\n",
        "    if col in merged_data.columns:\n",
        "        merged_data[col] = pd.to_datetime(merged_data[col], utc=True, errors='coerce')\n",
        "        print(f\"Missing values in {col} after conversion: {merged_data[col].isnull().sum()}\")\n",
        "\n",
        "# Check if all 'timestamp_dht' values are missing\n",
        "if merged_data['timestamp_dht'].isnull().all():\n",
        "    print(\"All 'timestamp_dht' values are missing after conversion.\")\n",
        "    # Investigate why and address accordingly\n",
        "else:\n",
        "    # Drop rows with missing 'timestamp_dht' if necessary\n",
        "    merged_data.dropna(subset=['timestamp_dht'], inplace=True)\n",
        "    print(\"Shape after dropping missing 'timestamp_dht':\", merged_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWxuowGFh6c1",
        "outputId": "27e207cc-84c4-49c3-dc78-fbb04d7d2f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after processing 'timestamp_dht': (11589006, 19)\n",
            "Total missing values in numeric columns before imputation: 117082628\n",
            "Performing MICE imputation on missing values...\n",
            "Total missing values in numeric columns after imputation: 0\n"
          ]
        }
      ],
      "source": [
        "# Proceed only if merged_data is not empty\n",
        "if not merged_data.empty:\n",
        "    # Convert 'timestamp_dht' to datetime\n",
        "    merged_data['timestamp_dht'] = pd.to_datetime(merged_data['timestamp_dht'], utc=True, errors='coerce')\n",
        "\n",
        "    # Retain only date information\n",
        "    merged_data['timestamp_dht'] = merged_data['timestamp_dht'].dt.floor('D')\n",
        "\n",
        "    # Drop rows with missing 'timestamp_dht' (none in your case)\n",
        "    merged_data.dropna(subset=['timestamp_dht'], inplace=True)\n",
        "\n",
        "    # Print the shape after processing 'timestamp_dht'\n",
        "    print(\"Shape after processing 'timestamp_dht':\", merged_data.shape)\n",
        "\n",
        "    # Clean numeric columns\n",
        "    numeric_cols = hrv_columns + sleep_columns + outcome_columns\n",
        "    available_numeric_cols = [col for col in numeric_cols if col in merged_data.columns]\n",
        "\n",
        "    # Convert columns to numeric type\n",
        "    merged_data[available_numeric_cols] = merged_data[available_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Check for missing values in numeric columns\n",
        "    total_missing = merged_data[available_numeric_cols].isnull().sum().sum()\n",
        "    print(f\"Total missing values in numeric columns before imputation: {total_missing}\")\n",
        "\n",
        "    # Proceed with MICE imputation\n",
        "    print(\"Performing MICE imputation on missing values...\")\n",
        "    mice_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "    merged_data[available_numeric_cols] = mice_imputer.fit_transform(merged_data[available_numeric_cols])\n",
        "\n",
        "    # Verify that missing values have been imputed\n",
        "    total_missing_after = merged_data[available_numeric_cols].isnull().sum().sum()\n",
        "    print(f\"Total missing values in numeric columns after imputation: {total_missing_after}\")\n",
        "else:\n",
        "    print(\"Merged data is empty after merging. Cannot proceed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUkQQX-MXh0G",
        "outputId": "b28425ac-c9d4-43ee-aa9f-f356b67660c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dht_data shape: (422127, 13)\n",
            "questionnaire_data shape: (7622, 7)\n",
            "Number of common participant IDs after normalization: 341\n",
            "Merged data shape after merging: (11589006, 19)\n"
          ]
        }
      ],
      "source": [
        "# 1. Check DataFrames are not empty\n",
        "print(\"dht_data shape:\", dht_data.shape)\n",
        "print(\"questionnaire_data shape:\", questionnaire_data.shape)\n",
        "\n",
        "# 2. Ensure 'participant_id' columns exist\n",
        "if 'participant_id' not in dht_data.columns:\n",
        "    print(\"Error: 'participant_id' column missing in dht_data\")\n",
        "if 'participant_id' not in questionnaire_data.columns:\n",
        "    print(\"Error: 'participant_id' column missing in questionnaire_data\")\n",
        "\n",
        "# 3. Normalize 'participant_id's\n",
        "dht_data['participant_id'] = dht_data['participant_id'].astype(str).str.strip().str.upper()\n",
        "questionnaire_data['participant_id'] = questionnaire_data['participant_id'].astype(str).str.strip().str.upper()\n",
        "\n",
        "# 4. Check for common 'participant_id's\n",
        "common_ids = set(dht_data['participant_id']).intersection(set(questionnaire_data['participant_id']))\n",
        "print(\"Number of common participant IDs after normalization:\", len(common_ids))\n",
        "\n",
        "# 5. Proceed with merging if common IDs exist\n",
        "if len(common_ids) > 0:\n",
        "    merged_data = pd.merge(\n",
        "        dht_data,\n",
        "        questionnaire_data,\n",
        "        on='participant_id',\n",
        "        how='inner'\n",
        "    )\n",
        "    print(\"Merged data shape after merging:\", merged_data.shape)\n",
        "else:\n",
        "    print(\"No common participant IDs found. Cannot proceed with merging.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVXu_r5VXhd1"
      },
      "outputs": [],
      "source": [
        "    # Define risk thresholds based on clinical guidelines\n",
        "    phq9_threshold = 10  # Moderate to severe depression\n",
        "    gad_threshold = 10   # Moderate to severe anxiety\n",
        "    pss4_threshold = 7   # High perceived stress\n",
        "\n",
        "    # Create binary risk labels\n",
        "    if 'phq9_total' in merged_data.columns:\n",
        "        merged_data['risk_label_phq9'] = np.where(merged_data['phq9_total'] >= phq9_threshold, 'Risk', 'No Risk')\n",
        "    if 'gad_total' in merged_data.columns:\n",
        "        merged_data['risk_label_gad'] = np.where(merged_data['gad_total'] >= gad_threshold, 'Risk', 'No Risk')\n",
        "    if 'pss4_total' in merged_data.columns:\n",
        "        merged_data['risk_label_pss'] = np.where(merged_data['pss4_total'] >= pss4_threshold, 'Risk', 'No Risk')\n",
        "\n",
        "    # Combine risk labels into a single risk category\n",
        "    merged_data['risk_category'] = merged_data[['risk_label_phq9', 'risk_label_gad', 'risk_label_pss']].apply(\n",
        "        lambda x: 'Risk' if any(label == 'Risk' for label in x) else 'No Risk', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kUgBNrCXhHJ"
      },
      "outputs": [],
      "source": [
        "    # Visualize the distribution of risk categories\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x='risk_category', data=merged_data)\n",
        "    plt.title('Distribution of Risk Categories')\n",
        "    plt.xlabel('Risk Category')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFchZPR2Xgy_"
      },
      "outputs": [],
      "source": [
        "    # Filter data for the study period (May 2nd to December 1st, 2020)\n",
        "    merged_data = merged_data[(merged_data['timestamp_dht'] >= '2020-05-02') &\n",
        "                              (merged_data['timestamp_dht'] <= '2020-12-01')]\n",
        "    # Sort data by timestamp for time-series analysis\n",
        "    merged_data = merged_data.sort_values(by='timestamp_dht')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TGM-MAH_spp"
      },
      "source": [
        "<a name=\"transformation\"></a>\n",
        "## 4. Data Transformation\n",
        "\n",
        "We will apply dynamic windowing and transform time-series data into images using Gramian Angular Field (GAF) for input into a Convolutional Neural Network (CNN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0XuFBVp_h3L"
      },
      "outputs": [],
      "source": [
        "# Function to process each participant's data and create GAF images\n",
        "def process_participant(participant_data, metric, window_size=7):\n",
        "    X_gaf = []\n",
        "    y_gaf = []\n",
        "    # Iterate over the data using a sliding window approach\n",
        "    for start in range(0, len(participant_data) - window_size + 1, 1):\n",
        "        end = start + window_size\n",
        "        window_data = participant_data.iloc[start:end]\n",
        "        window_metric = window_data[metric].values\n",
        "        if np.any(np.isnan(window_metric)):\n",
        "            continue  # Skip if any values are missing in the window\n",
        "        # Apply Gramian Angular Field transformation\n",
        "        gaf = GramianAngularField(image_size=min(window_size, 32), method='summation')\n",
        "        gaf_image = gaf.fit_transform(window_metric.reshape(1, -1))\n",
        "        # Resize the image to 32x32 pixels\n",
        "        resized_gaf_image = resize(gaf_image[0], (32, 32), anti_aliasing=True)\n",
        "        X_gaf.append(resized_gaf_image)\n",
        "        # Determine the majority risk label in the window\n",
        "        window_risk_labels = window_data['risk_category']\n",
        "        majority_label = window_risk_labels.mode()[0]\n",
        "        y_gaf.append(1 if majority_label == 'Risk' else 0)\n",
        "    return X_gaf, y_gaf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fTJP_dP_hzf"
      },
      "outputs": [],
      "source": [
        "# Function to create GAF images for all participants using sliding windows\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def create_sliding_window_gaf(data, metric, window_size=7, batch_size=20000):\n",
        "    participants = data['participant_id'].unique()\n",
        "    X_gaf = []\n",
        "    y_gaf = []\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(process_participant, data[data['participant_id'] == participant], metric, window_size) for participant in participants]\n",
        "        for future in futures:\n",
        "            X_batch, y_batch = future.result()\n",
        "            X_gaf.extend(X_batch)\n",
        "            y_gaf.extend(y_batch)\n",
        "            # Yield batches to manage memory usage\n",
        "            if len(X_gaf) >= batch_size:\n",
        "                yield np.array(X_gaf), np.array(y_gaf)\n",
        "                X_gaf, y_gaf = [], []\n",
        "    # Yield any remaining data\n",
        "    if X_gaf:\n",
        "        yield np.array(X_gaf), np.array(y_gaf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nlass0C_hwM"
      },
      "outputs": [],
      "source": [
        "# Generate GAF images for the 'score_hrv_balance' metric\n",
        "X_gaf_hrv_balance = []\n",
        "y_gaf_hrv_balance = []\n",
        "for X_batch, y_batch in create_sliding_window_gaf(merged_data, metric='score_hrv_balance'):\n",
        "    X_gaf_hrv_balance.append(X_batch)\n",
        "    y_gaf_hrv_balance.append(y_batch)\n",
        "# Concatenate all batches into a single array\n",
        "X_gaf_hrv_balance = np.concatenate(X_gaf_hrv_balance, axis=0)\n",
        "y_gaf_hrv_balance = np.concatenate(y_gaf_hrv_balance, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omvwz_Pc_4_a"
      },
      "source": [
        "<a name=\"modeling\"></a>\n",
        "## 5. Machine Learning Model Development\n",
        "\n",
        "We will develop a CNN model to classify the risk of mental health issues based on the transformed GAF images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4_7HutR_hsG"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset to ensure randomness\n",
        "X_gaf_hrv_balance, y_gaf_hrv_balance = shuffle(X_gaf_hrv_balance, y_gaf_hrv_balance, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfgreuFW_hoX"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train_hrv_balance, X_test_hrv_balance, y_train_hrv_balance, y_test_hrv_balance = train_test_split(\n",
        "    X_gaf_hrv_balance, y_gaf_hrv_balance, test_size=0.2, random_state=42, stratify=y_gaf_hrv_balance\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9zhre8W__ic"
      },
      "outputs": [],
      "source": [
        "# Reshape data for input into the CNN (add channel dimension)\n",
        "X_train_hrv_balance = X_train_hrv_balance.reshape(-1, 32, 32, 1).astype('float32')\n",
        "X_test_hrv_balance = X_test_hrv_balance.reshape(-1, 32, 32, 1).astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NiEkYQJ__a2"
      },
      "outputs": [],
      "source": [
        "# Define the Convolutional Neural Network (CNN) architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)),  # First convolutional layer\n",
        "    MaxPooling2D(2, 2),  # First pooling layer\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer\n",
        "    MaxPooling2D(2, 2),  # Second pooling layer\n",
        "    Flatten(),  # Flatten the feature maps into a 1D vector\n",
        "    Dense(128, activation='relu'),  # Fully connected layer\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKHdmMhY__Uu"
      },
      "outputs": [],
      "source": [
        "# Compile the model with optimizer, loss function, and metrics\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grj0JW2G__PP"
      },
      "outputs": [],
      "source": [
        "# Define callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE6JOnVM__KF"
      },
      "outputs": [],
      "source": [
        "# Train the model with validation data and callbacks\n",
        "history = model.fit(X_train_hrv_balance, y_train_hrv_balance,\n",
        "                    epochs=30,\n",
        "                    validation_data=(X_test_hrv_balance, y_test_hrv_balance),\n",
        "                    batch_size=64,\n",
        "                    callbacks=[early_stopping, reduce_lr])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JesR5mzS__Cf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_hrv_balance, y_test_hrv_balance)\n",
        "print(f'HRV Balance Model - Test Accuracy: {test_accuracy}, Loss: {test_loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLMz-9kjAPb6"
      },
      "source": [
        "<a name=\"evaluation\"></a>\n",
        "## 6. Model Evaluation and Visualization\n",
        "\n",
        "We will evaluate the model using metrics like confusion matrix, classification report, ROC curve, and visualize the training history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3zsshXT_-93"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on the test set\n",
        "y_pred_hrv_balance_probs = model.predict(X_test_hrv_balance)\n",
        "y_pred_hrv_balance = (y_pred_hrv_balance_probs > 0.5).astype(int).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF__ztTHAUYE"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix and classification report\n",
        "conf_matrix_hrv_balance = confusion_matrix(y_test_hrv_balance, y_pred_hrv_balance)\n",
        "print(\"\\nConfusion Matrix (HRV Balance):\")\n",
        "print(conf_matrix_hrv_balance)\n",
        "class_report_hrv_balance = classification_report(y_test_hrv_balance, y_pred_hrv_balance)\n",
        "print(\"\\nClassification Report (HRV Balance):\")\n",
        "print(class_report_hrv_balance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YWMx_adAWgv"
      },
      "outputs": [],
      "source": [
        "# Visualize training and validation metrics\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='green', linestyle='--')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green', linestyle='--')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6EK6DiAYlb"
      },
      "outputs": [],
      "source": [
        "# ROC Curve and AUC\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, thresholds = roc_curve(y_test_hrv_balance, y_pred_hrv_balance_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_9vSJOAc2y"
      },
      "outputs": [],
      "source": [
        "# Precision, Recall, F1-Score Visualization\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "precision = precision_score(y_test_hrv_balance, y_pred_hrv_balance)\n",
        "recall = recall_score(y_test_hrv_balance, y_pred_hrv_balance)\n",
        "f1 = f1_score(y_test_hrv_balance, y_pred_hrv_balance)\n",
        "\n",
        "# Bar chart for performance metrics\n",
        "metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "values = [precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=metrics, y=values, palette='Blues_d')\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylim(0, 1)\n",
        "for i, value in enumerate(values):\n",
        "    plt.text(i, value + 0.02, f\"{value:.2f}\", ha='center')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1SUfL5KAiw8"
      },
      "source": [
        "<a name=\"interpretability\"></a>\n",
        "## 7. Model Interpretability\n",
        "\n",
        "We will use SHAP values, LIME, and saliency maps to interpret the model's predictions and understand the important features contributing to the decision-making process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BliHEHeeAcuJ"
      },
      "outputs": [],
      "source": [
        "# SHAP Values Visualization\n",
        "\n",
        "# Due to computational constraints, we'll use a small subset\n",
        "X_test_sample = X_test_hrv_balance[:100]\n",
        "y_test_sample = y_test_hrv_balance[:100]\n",
        "\n",
        "# Create a DeepExplainer object for the trained model\n",
        "explainer = shap.DeepExplainer(model, X_train_hrv_balance[:100])\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "\n",
        "# Visualize the SHAP values for a sample of test data\n",
        "shap.image_plot(shap_values, -X_test_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVNpWEO6Acln"
      },
      "outputs": [],
      "source": [
        "# LIME Interpretation\n",
        "\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "# Convert grayscale images to RGB (duplicate channels) for LIME\n",
        "X_test_rgb = np.repeat(X_test_hrv_balance[:10], 3, axis=-1)\n",
        "\n",
        "# Initialize LIME image explainer\n",
        "lime_explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Function to make predictions for LIME\n",
        "def predict_fn(images):\n",
        "    images = images[..., 0].reshape(-1, 32, 32, 1)\n",
        "    return model.predict(images)\n",
        "\n",
        "# Explain a single image\n",
        "explanation = lime_explainer.explain_instance(\n",
        "    X_test_rgb[0].astype('double'),\n",
        "    predict_fn,\n",
        "    top_labels=1,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "# Get image and mask from the explanation\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    explanation.top_labels[0],\n",
        "    positive_only=True,\n",
        "    num_features=5,\n",
        "    hide_rest=False\n",
        ")\n",
        "\n",
        "# Display the explanation\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
        "plt.title('LIME Explanation')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkwU-e7aAcfN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku_tEodMAcCZ"
      },
      "outputs": [],
      "source": [
        "# Saliency Maps using tf-keras-vis\n",
        "\n",
        "# Define a model modifier to replace the last activation with linear\n",
        "def model_modifier_function(cloned_model):\n",
        "    cloned_model.layers[-1].activation = tf.keras.activations.linear\n",
        "\n",
        "# Create a Saliency object\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=model_modifier_function,\n",
        "                    clone=True)\n",
        "\n",
        "# Define loss function for saliency\n",
        "def loss_function(output):\n",
        "    return output[:, 0]\n",
        "\n",
        "# Generate saliency maps for a subset of test data\n",
        "saliency_maps = saliency(loss_function, X_test_hrv_balance[:10])\n",
        "saliency_maps = np.abs(saliency_maps)\n",
        "\n",
        "# Plot original images and their corresponding saliency maps\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(10):\n",
        "    # Original image\n",
        "    plt.subplot(2, 10, i + 1)\n",
        "    plt.imshow(X_test_hrv_balance[i].squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    # Saliency map\n",
        "    plt.subplot(2, 10, i + 11)\n",
        "    plt.imshow(saliency_maps[i].squeeze(), cmap='hot')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Original Images and Saliency Maps')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEEWWboOAw11"
      },
      "source": [
        "<a name=\"dimensionality\"></a>\n",
        "## 8. Alternative Modeling with Dimensionality Reduction\n",
        "\n",
        "We will explore alternative modeling approaches by applying Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xndj-mB1Axsb"
      },
      "outputs": [],
      "source": [
        "# Flatten the GAF images for PCA\n",
        "X_flat = X_gaf_hrv_balance.reshape(X_gaf_hrv_balance.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4XD3iZnA0Gr"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to reduce dimensionality\n",
        "n_components = 50\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_flat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCmzY4XNA17t"
      },
      "outputs": [],
      "source": [
        "# Visualize explained variance\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC4ewCAvA5C2"
      },
      "outputs": [],
      "source": [
        "# t-SNE Visualization\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
        "X_tsne = tsne.fit_transform(X_pca)\n",
        "\n",
        "# Plot t-SNE result\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_gaf_hrv_balance, palette='viridis')\n",
        "plt.title('t-SNE Visualization of GAF Images')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.legend(title='Risk Label', loc='best')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GFqHIofA8s5"
      },
      "outputs": [],
      "source": [
        "# Train a model on PCA-transformed data\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "    X_pca, y_gaf_hrv_balance, test_size=0.2, random_state=42, stratify=y_gaf_hrv_balance\n",
        ")\n",
        "\n",
        "# Define a simple neural network\n",
        "model_pca = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(n_components,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_pca.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_pca = model_pca.fit(X_train_pca, y_train_pca, epochs=20, validation_data=(X_test_pca, y_test_pca), batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EKSvR4_A8bE"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss_pca, test_accuracy_pca = model_pca.evaluate(X_test_pca, y_test_pca)\n",
        "print(f'PCA Model - Test Accuracy: {test_accuracy_pca}, Loss: {test_loss_pca}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcFXDf6eA8S8"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "y_pred_pca_probs = model_pca.predict(X_test_pca)\n",
        "y_pred_pca = (y_pred_pca_probs > 0.5).astype(int).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gUWVH3bA8LZ"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and classification report\n",
        "conf_matrix_pca = confusion_matrix(y_test_pca, y_pred_pca)\n",
        "print(\"\\nConfusion Matrix (PCA Model):\")\n",
        "print(conf_matrix_pca)\n",
        "class_report_pca = classification_report(y_test_pca, y_pred_pca)\n",
        "print(\"\\nClassification Report (PCA Model):\")\n",
        "print(class_report_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVN8R05xA8D-"
      },
      "outputs": [],
      "source": [
        "# Plot training history for PCA model\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_pca.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "plt.plot(history_pca.history['val_accuracy'], label='Validation Accuracy', color='green', linestyle='--')\n",
        "plt.title('Training vs Validation Accuracy (PCA Model)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_pca.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history_pca.history['val_loss'], label='Validation Loss', color='green', linestyle='--')\n",
        "plt.title('Training vs Validation Loss (PCA Model)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsRN9Cp_BOVR"
      },
      "source": [
        "<a name=\"comparison\"></a>\n",
        "## 9. Comparison of Models with Different PCA Components\n",
        "\n",
        "We will compare the performance of models trained with different numbers of PCA components to understand the impact of dimensionality on model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgIrZUNCBTFg"
      },
      "outputs": [],
      "source": [
        "# Function to train model with different number of PCA components\n",
        "def train_model_with_pca_components(n_components_list, X_flat, y):\n",
        "    results = {}\n",
        "    for n_components in n_components_list:\n",
        "        # Apply PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_pca = pca.fit_transform(X_flat)\n",
        "        # Split data\n",
        "        X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "            X_pca, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        # Define model\n",
        "        model_pca = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(n_components,)),\n",
        "            Dropout(0.5),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        # Compile model\n",
        "        model_pca.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        # Train model\n",
        "        history_pca = model_pca.fit(X_train_pca, y_train_pca, epochs=20, validation_data=(X_test_pca, y_test_pca), batch_size=32, verbose=0)\n",
        "        # Evaluate model\n",
        "        test_loss_pca, test_accuracy_pca = model_pca.evaluate(X_test_pca, y_test_pca, verbose=0)\n",
        "        results[n_components] = test_accuracy_pca\n",
        "        print(f'Components: {n_components}, Test Accuracy: {test_accuracy_pca}')\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkNhigvABT-C"
      },
      "outputs": [],
      "source": [
        "# Test different numbers of PCA components\n",
        "n_components_list = [20, 30, 40, 50, 60]\n",
        "results = train_model_with_pca_components(n_components_list, X_flat, y_gaf_hrv_balance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe4kDadGBTz5"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy vs number of PCA components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_components_list, [results[n] for n in n_components_list], marker='o', linestyle='-', color='blue')\n",
        "plt.title('Model Accuracy vs Number of PCA Components')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRTFJsxWBaqc"
      },
      "source": [
        "<a name=\"integrity\"></a>\n",
        "## 10. Data Integrity and Minimization\n",
        "\n",
        "Data minimization is addressed by focusing on HRV metrics, which were identified as the most predictive and complete data available. This reduces the amount of data processed and stored, enhancing privacy and complying with data protection principles. Additionally, the use of sliding windows and GAF transformation condenses the time-series data into compact representations suitable for modeling, further contributing to data minimization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
